%\section{Introduction to network file systems}
\section{Introduction}

For quite a while now, files have been the way to persistently store information in a computer system
from both the applications' and users' point of view.
For instance, even back in the days of DOS the operating system provided built-in support
for file (and later, directories) management on floppies and hard drives.
The user could use commands like \texttt{dir} or \texttt{del} to manipulate their
files and applications were provided with a standard APIs to work with files.
The typical way of actually storing the data is on a persistent storage media,
like a hard drive, floppy disk or USB stick that is directly attached to the computer.
An alternative way is to introduce computer networking into the mix, leading to
network file systems, bringing a new dimension of capabilities and challenges to file
management on the table.

In chapter~\ref{sect:NetworkFileSystems} we will discuss the capabilities
and benefits of network file systems and then concentrate on NFS and SMB/CIFS, two common
network file system protocols.
Then we will show various ways of performing measurements on network file systems.
Chapter~\ref{sect:PerfMeasurements} describes general performance benchmarking methodology
using the Postmark and LADDIS tools and in chapter~\ref{sect:NFSv41} we will focus
on a case study on comparing performance between different versions of the NFS protocol.
Finally, we will show a different sort of measurement in chapter~\ref{sect:NetAppAnalysis}
where metrics and usage patterns of a real-world network file server are analyzed.

\section{Network file systems} \label{sect:NetworkFileSystems}
Network file systems allow files on a server to be managed and accessed over the network.
Several different protocols for network file systems have been designed over the years,
with the most well-known being Network File System (NFS) and 
Server Message Block (SMB).
% Common Internet File System (CIFS).
Though history and specifics of those protocols differ considerably, the common design
goal shared by both of them is to be transparent to the applications using them.
That is, applications and libraries should work identically on networked file systems
as they work on local file systems, without requiring any code changes or recompilation.

\subsection{NFS}

The NFS protocol originates from Sun Microsystems, where the initial implementation work for the
Unix 4.2 operating system was started in 1984~\cite{NFS}. Its original design goals were
transparent operation with existing programs by maintaining existing Unix file system
semantics, ability to recover from server reboots or crashes, and having reasonable
performance.
The first released version of NFS was included in SunOS 2.0
% TODO cite solaris book
and while otherwise functional,
file locking was not supported and certain behaviors,
namely append mode writes and removing still-open files were only partially implemented.
The first published version of the protocol was version 2 of the protocol, commonly abbreviated \emph{NFSv2}.
It used UDP as the transport layer protocol and was completely stateless.
This allows a simple method of handling server crashes: a client can simply keep re-sending
its pending NFS requests until the server is restarted and the client gets a reply back.
Since then, extensions to the original protocol have been standardised
in several RFCs.
The NFSv3 specification~\cite{NFSv3RFC} was published in 1995 and provided improvements
like 64-bit file sizes, operation over TCP instead of UDP and some more featureful protocol
requests for increased performance.
In turn, NFSv4~\cite{NFSv4RFC} and NFSv4.1~\cite{NFSv41RFC} turn the protocol into a stateful one,
allowing for a whole slew of features such as built-in file locking support, better security
and better control over client-side caching~\cite{NFSv4Better}.
We will have a closer look at measuring the improvements provided by NFSv4.1 in chapter~\ref{sect:NFSv41}.

\subsection{SMB / CIFS}

The SMB protocol has a more complex history behind it. The protocol was initially designed
and implemented by IBM for the DOS operating system in the 80s
and ran over NetBIOS on the session layer and NBF (NetBIOS frames) on the network and transfer layers~\cite{CifsBook}.
In addition to being a file server, protocol, it is capable of sharing printers and serial ports over the network as well.
Further developments to the protocol were made by Microsoft.
In 1992 parts of the the protocol were specified in an X/Open standard~\cite{XopenSmbSpec}.
In 1996, Microsoft started using the Common Internet File System (CIFS) name for the protocol.

TODO: more stuff here.

\section{Performance measurements} \label{sect:PerfMeasurements}

As mentioned earlier, good performance was one of the design goals of the NFS protocol.
As a result, various benchmarks for NFS server implementations have been implemented.

\subsection{Postmark}
As most network file systems permit applications to transparently work with them,
a simple way of benchmarking network file system implementations is to run regular file system benchmarks over them.
One such instance is the Postmark benchmark, which is designed to have a workload similar to a mail or newsgroup server.
The benchmark exercises the performance of handling small files by simulating operations of a server which
stores mail on the disk as one file per message.
% The benchmark exercises the performance of handling small files by simulating operations on multiple "mailboxes"
% that are directories containing a large number of messages, each with its own file.
% FIXME: directories only came in later versions

The test starts with a number of initial files containing a random amount of data according to a configurable uniform random distribution.
Then a number of transactions are performed on the files; each transaction consists of two randomly selected operations.
The first operation is either a file creation or a file deletion, the second is either reading from a file or appending to a file.
For each operation, an random file is picked, independent of what while was picked for the other operation of the transaction.
File creations also write a random amount of initial content to the selected file.
Amount of data in the files (during initial creation and create or append operations) is kept within configurable limits.
There are also a handful of other configurable parameters, like initial number of files and ratio of operations to perform.
The pseudo-random number generator is fixed, so the benchmark with the same parameters should work identically on different systems.
Results from the benchmark consists of details on both the overall benchmark run and details for each operation type.
The rate of transactions per second in total seems to be the primary metric of the benchmark.
For each operation type, the number and rate of operations of that type is reported.
Additionally for the read and write operations, throughput numbers (in bytes/second) are provided as well.

The Postmark authors themselves have numbers comparing performance numbers for comparing local file systems on both Unix and Windows
to specialised NetApp storage appliances connected over NFS and CIFS, respectively.
The Unix and Windows servers in the test were running on different hardware and file systems,
with details of the NetApp hardware not disclosed.
The results claim a transaction rate of 2x-3x for the networked storage compared to local file systems.
While this sort of benchmark is somewhat questionable as the disks and other underlying storage hardware aren't equivalent,
it does show that the neither the network file system protocols nor clients in either operating system have
bottlenecks that would limit the performance.

The benchmark has been popular in both regular and networked file system benchmarks;
for instance the study ``A Nine Year Study of File System and Storage Benchmarking''~\cite{MetaStudy} from 2008
surveyed 106 file system and storage related papers from years 1999--2006, and Postmark was used in 30 papers.
However, it also raises some criticism on the benchmark.
In particular, the workload of the benchmark doesn't scale well with the increases in the speed of hardware;
in one extreme case the benchmark completed in under a tenth of a second on modern hardware when the default parameters
of the benchmark were used.
Since the defaults for the benchmark run for a too short time to give accurate results,
the papers' authors picked their own parameters.
This makes it difficult to compare Postmark results to others' results,
or even non-reproducible, as many papers failed to report the parameters used in the necessary degree.

\subsection{LADDIS}
One slightly different kind of a benchmark for testing NFS exclusively is LADDIS from the 90s~\cite{LADDIS}, designed and named after by
several NFS server vendors at that time (\underline{L}egato Systems, \underline{A}uspex Systems, \underline{D}ata General,
\underline{D}igital Equipment, \underline{I}nterphase and \underline{S}un Microsystems).
It is designed to be a standard benchmark for making performance comparisons across different NFS server implementations or configurations.
The LADDIS benchmarking software is a synthetic workload generator that talks to the NFS server directly,
without any dependency on the NFS client implementation of the client system running the benchmark.
This is an important design choice to keep the benchmark focused on the NFS server implementation and makes the results more comparable
between different clients used for testing.
For instance, different clients might generate slightly different protocol level requests for the same high-level file system operation,
and details of client-side caching could affect the results.
One downside of this approach is that the benchmark can't be run on newer versions of the protocol without modifications;
the initial version only supported NFSv2 over UDP~\cite{MetaStudy}.
Another trait of the benchmark that is specific to network file system benchmarks is that the benchmark can be run with
multiple client systems generating load on the server.
For that, the software is split to separate manager and load generation programs that can communicate over the TCP protocol.

TODO: continue this.
% The implementation of the benchmark consists of making randomly selected NFS protocol operations
% (for instance, creating deleting or reading from a file)
% to the server.
% The distribution of which operations to perform

\section{Performance differences between protocol revisions} \label{sect:NFSv41}

As was mentioned earlier, good performance was one of the design goals for the NFS protocol,
and since the original protocol implementation additional efforts have been spent improving the performance of the protocol.
Naturally, new versions of the protocol raise the question of how much does an improved protocol revision actually help.
For version 4.1 of the NFS protocol, this has been investigated in the aptly-named article ``Newer Is Sometimes Better: An Evaluation of NFSv4.1''~\cite{NFSv4Better}, which compares NFSv3, NFSv4 and NFSv4.1 performance of the Linux NFS server implementation.
Their test setup was done on a 10GbE network with five client machines and one server machine, running identical enterprise-class hardware.
The NFS server was equipped with fast solid-state disks in RAID-0 to keep the focus of the tests on the protocol level performance.
In addition to comparing performance with multiple clients, the test setup included possibility of adding artificial network latency to the test.

\subsection{NFSv4.1 delegations}
According on the team's analysis on their results, a significant improvement that the version 4.1 of the NFS protocol brings is the \emph{delegation
mechanism} to improve the effectiveness of client-side caching.
Consider an implementation that wishes to do read caching and simultaneously maintain coherency in case of other clients
concurrently writing to the same file.
In the stateless NFSv3 protocol, the client doing the reads must periodically ask the server for the last-modification time stamp of the file in order to notice other clients writing to the file.
The read delegation feature of NFSv4.1 allows a better way of managing caching.
The server now maintains state for opened files, so a lone client reading files can proceed with full caching,
and in case some other client starts writing concurrently, the server will notify the client and recall the delegation.

For concrete numbers on the benefits of delegations,
figure \ref{fig:nfsv41randread} shows results of a microbenchmark with 5 clients repeatedly reading small files for 5 minutes.
Files are randomly selected from a set of 10 000 small files of 4KB each.
NFSv3 starts off being faster, but once the speeds of both protocols have stabilised,
NFSv4.1 has reached 2x the throughput of NFSv3.
On the protocol level, NFSv3 performed 8.3$\times$ more requests compared to NFSv4.1,
where 95\% of those requests were GETATTR operations made for cache invalidation.

However, the delegations feature is not an universal improvement.
The need for OPEN and CLOSE requests to maintain state of open files on the server can significantly increase the amount of protocol requests made by NFSv4.1 compared to NFSv3.
For instance, the file server workload of the Filebench benchmark caused an increase of 56\% in the number of protocol requests made by NFSv4.1 over NFSv3.
This translated into a 8--18\% of slowdown.
% The biggest difference was in the zero-delay network, where V4.1p was 15% slower

\begin{figure}[h]
\centering\includegraphics[width=0.5\textwidth]{images/nfsv41better-reading-small-files.png}
\caption{Figure 6 from ``Newer Is Sometimes Better: An Evaluation of NFSv4.1''~\cite{NFSv4Better}.}
% Section 4.1 Read Small Files
\label{fig:nfsv41randread}
\end{figure}

\section{Analysis of a production file server} \label{sect:NetAppAnalysis}

Another approach of analysing network file systems is to look at how the actual users of such systems are using it.
One such example comes from ``Measurement and Analysis of Large-scale Network File System Workloads''~\cite{NetApp} done by NetApp,
a company developing network storage products.
The study was done on the corporate's own internal file servers by collecting live packet captures of CIFS protocol traffic.
Data was collected from two different servers, one serving users and one serving engineering users, creating two different data sets.
Compared to other ways of collecting data on file server usage, such as analysing static file system snapshots of the file server,
the use of network traffic as a data source allows analysis of changes to the file system over time.
Since user authentication in the test environment was done by a separate Kerberos protocol,
only IP addresses were used to correlate CIFS sessions to actual unique users of the system.

\subsection{I/O amount analysis}

\begin{table}[h!]

\label{tab:NetAppData}
\begin{tabu}{r|[1pt]c|c}
    Dataset & Corporate & Engineering \\
    \tabucline[1pt]{-}
    Clients                     & 5261      & 2654      \\
    Disk space in use (TB)      & 3         & 19        \\
    Duration (days)             & 65        & 97        \\
    Packet capture size (GB)    & 750       & 1500      \\
    \hline
    Data read (GB)              & 364.3     & 723.4     \\
    - per day (GB)              & 5.60      & 7.46      \\
    Data written (GB)           & 177.7     & 364.4     \\
    - per day (GB)              & 2.73      & 3.76      \\
    \hline
    R/W operation ratio         & 3.2       & 2.3       \\
    R/W byte ratio              & 2.1       & 2.0       \\
\end{tabu}

\caption{Overview of file data I/O in the NetApp datasets~\cite{NetApp}.}

\end{table}

Table~\ref{tab:NetAppData} shows basic statistics on the amount of data I/O performed in the datasets.
% TODO complete garbage, it's 70%
From the network point of view, a surprisingly low amount of data transferred is for the actual file data, only 30\%,
making majority of the traffic being spent in both metadata accesses and protocol overheads.
On the protocol level, 21\% of the CIFS requests were for file I/O.
It's interesting to compare these numbers to the total disk usage of the system:
only a relatively small amount of it was accessed during the measurement period.
By purely looking at the file data I/O numbers,
the corporate and engineering datasets had only 18\% and 6\% of the
in-use disk capacity touched in the three-month period.
And even these numbers don't account for files that were accessed multiple times.
This suggests that a large amount of data in the measured workload is for archival purposes,
i.e. written once and then left alone for most of the time.
This observation is confirmed by analyzing the amount of times a file is reopened on a per-file level.
For the observed workload, 65\% of files are only ever opened once and 94\% are opened less than five times.
Furthermore, re-opens of a file often occur in a short timespan and are correlated to each other,
as a majority of file re-opens happen less than one minute after the file has been closed.
From a storage server designer's point of view,
these observations suggest a number of improvements that could be made on several layers of the system.
The authors suggest that once the infrequently accessed files could be identified,
they could be compressed or moved to a lower-tier storage,
with only minor loss of performance compared to the benefits.
The lower-tier disks could be selected to have a worse write performance
but better storage density or less power consumption, for instance.

\subsection{I/O pattern analysis}
Continuing on the numbers in table~\ref{tab:NetAppData},
both datasets have shown a ratio of 2:1 for the read:write byte ratio.
So for each GB of file data written, roughly 2 GB of bytes read were observed.
The authors noted that the result was unexpectedly low when compared to results from other file system studies.
Older studies from 2000 and earlier had shown R/W ratios of 4:1 or higher.
It was speculated that improved client-side caching done by operating systems
has significantly reduced the amount of reads.
Additionally, network file systems are not expected to contain as large fraction of read-heavy system files as local file systems do.

Overall, these observations about increased write amounts suggests new possibilities for improving file servers.
Specifically, filesystems optimized for write-heavy workloads like WAFL~\cite{WAFL} and Sprite LFS~\cite{SpriteLfs} are recommended.

\subsection{File sharing}

Previously in section \ref{sect:NFSv41} we discussed the delegations feature NFSv4.1 which would improve performance in the case of files not being concurrently accessed by multiple clients.
In the NetApp data sets, even non-concurrent sharing of files was relatively uncommon: 23.9\% and 2.9\% files were ever opened by user.
And with concurrent sharing, the numbers drop down to 7.3\% and 0.3\% respectively.
So the trade-offs involved in the NFSv4.1 delegations feature seem justified by these results.
Although the result was obtained in a CIFS environment instead of NFS.
the authors claim that the results are not unique to CIFS.

\section{Conclusions}
In this paper we have discussed network filesystems.
They have clear benefits (such as simplified backups) in large computing environments such as corporate networks or universities
and the performance is comparable to local disks.
NFS and SMB/CIFS are widely-used network file system protocols.
Even though their origins are different, with NFS coming from the Unix world
and SMB/CIFS from the DOS/Windows world, ultimately they both offer very similar functionality.
In particular, both of them are ``transparent'' and allow applications to work on files over the network
exactly like they would work on local files.

We have shown different kinds of measurements for network file systems.
One aspect we've shown is benchmarking.
Good performance is a a common design goal for network file system protocols,
so both users and developers of network file systems want to know the system performs as expected.
We've shown two benchmarks that achieve this via different means:
Postmark for a workload simulating a real-word server use case,
and LADDIS for a protocol-level microbenchmark of a NFS server.

Both of them are complex by the nature of having several protocol revisions developed over the previous three decades.
Improvements made in newer protocol revisions (NFSv4.1 for example) can boost performance in common cases,
but those optimisations have trade-offs that can make other use cases slower as well.
